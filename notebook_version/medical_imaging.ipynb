{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Imaging & Vesuvius Challenge Training Notebook\n",
    "\n",
    "**Notebook Version** - Self-contained notebook with duplicated data folder.\n",
    "\n",
    "This notebook is adapted from `train.py` to run on Kaggle. It supports:\n",
    "\n",
    "## Medical Image Classification\n",
    "- **Classification** across multiple medical imaging domains (brain, lungs, skin, breast, bone)\n",
    "- **Segmentation** with a UNet model\n",
    "\n",
    "## Vesuvius Challenge (3D CT Volume Segmentation)\n",
    "- **3D volume segmentation** for ancient scroll CT scans\n",
    "- **Competition metrics**: Surface Dice, TopoScore, VOI\n",
    "- **.tif volume I/O** with variable dimension handling\n",
    "- **2.5D UNet** with multi-slice context\n",
    "\n",
    "All code is self-contained - no external Python files needed!\n",
    "\n",
    "## Required Dependencies\n",
    "\n",
    "**Automatic Installation**: Run cell 0 (Install Required Packages) to automatically install all dependencies.\n",
    "\n",
    "**Manual Installation** (if needed):\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "pip install tifffile scipy scikit-image\n",
    "pip install pillow numpy\n",
    "```\n",
    "\n",
    "**Note**: The notebook includes fallback handling if some dependencies are missing, but full functionality requires all packages. After installing packages, restart the kernel and run all cells.\n",
    "\n",
    "## Data Directory Setup\n",
    "\n",
    "The notebook uses a configurable `DATA_ROOT` directory (set in cell 2) that works across different environments:\n",
    "\n",
    "- **Local PC**: `DATA_ROOT = 'data'` (default)\n",
    "- **Kaggle**: Set `DATA_ROOT = '/kaggle/input/vesuvius-challenge-surface-detection'` or use environment variable\n",
    "- **Google Colab**: Set `DATA_ROOT = '/content/data'` or mount your drive\n",
    "- **Cloud/Remote**: Set via environment variable: `export DATA_ROOT=/path/to/data`\n",
    "\n",
    "All data paths in the notebook are relative to `DATA_ROOT`, making it easy to switch between environments.\n",
    "\n",
    "---\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this notebook for the Vesuvius Challenge, please cite:\n",
    "\n",
    "```bibtex\n",
    "@misc{vesuvius-challenge-surface-detection,\n",
    "    author = {Sean Johnson and David Josey and Elian Rafael Dal PrÃ  and Hendrik Schilling and Youssef Nader and Johannes Rudolph and Forrest McDonald and Paul Henderson and Giorgio Angelotti and Sohier Dane and MarÃ­a Cruz},\n",
    "    title = {Vesuvius Challenge - Surface Detection},\n",
    "    year = {2025},\n",
    "    howpublished = {\\url{https://kaggle.com/competitions/vesuvius-challenge-surface-detection}},\n",
    "    note = {Kaggle}\n",
    "}\n",
    "```\n",
    "\n",
    "**Competition**: [Vesuvius Challenge - Surface Detection](https://kaggle.com/competitions/vesuvius-challenge-surface-detection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Required Packages\n",
    "\n",
    "**Run this cell first** to automatically install all required dependencies.\n",
    "\n",
    "- âœ… **Kaggle**: Most packages are pre-installed, but this will install any missing ones\n",
    "- âœ… **Google Colab**: Will install all required packages\n",
    "- âœ… **Local Jupyter**: Will install packages in your current environment\n",
    "- âš ï¸  **After installation**: Restart the kernel and run all cells for imports to work\n",
    "\n",
    "You can skip this cell if all packages are already installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Checking and installing required packages...\n",
      "============================================================\n",
      "âœ… torch is already installed\n",
      "âœ… torchvision is already installed\n",
      "âœ… tifffile is already installed\n",
      "âœ… scipy is already installed\n",
      "ðŸ“¦ Installing scikit-image...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Failed to install scikit-image\n",
      "âœ… pillow is already installed\n",
      "âœ… numpy is already installed\n",
      "============================================================\n",
      "Package installation complete: 6/7 packages ready\n",
      "============================================================\n",
      "\n",
      "âš ï¸  Note: If packages were just installed, you may need to restart the kernel\n",
      "   and run the cells again for imports to work properly.\n",
      "   In Jupyter: Kernel -> Restart & Run All\n",
      "   In Kaggle/Colab: Runtime -> Restart runtime\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# This cell will install all dependencies needed for the notebook\n",
    "# You can skip this if packages are already installed\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(pip_name, import_name=None):\n",
    "    \"\"\"\n",
    "    Install a package using pip, with error handling.\n",
    "    \n",
    "    Args:\n",
    "        pip_name: Package name for pip install (e.g., 'scikit-image')\n",
    "        import_name: Import name if different (e.g., 'skimage'). If None, uses pip_name\n",
    "    \"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = pip_name\n",
    "    \n",
    "    # Check if already installed\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"âœ… {pip_name} is already installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"ðŸ“¦ Installing {pip_name}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name, \"--quiet\"])\n",
    "            # Try importing again to verify\n",
    "            try:\n",
    "                __import__(import_name)\n",
    "                print(f\"âœ… {pip_name} installed successfully\")\n",
    "                return True\n",
    "            except ImportError:\n",
    "                print(f\"âš ï¸  {pip_name} installed but import '{import_name}' failed - may need kernel restart\")\n",
    "                return True  # Still return True as package was installed\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"âŒ Failed to install {pip_name}\")\n",
    "            return False\n",
    "\n",
    "# List of required packages: (pip_name, import_name)\n",
    "# Some packages have different pip names vs import names\n",
    "required_packages = [\n",
    "    (\"torch\", \"torch\"),\n",
    "    (\"torchvision\", \"torchvision\"),\n",
    "    (\"tifffile\", \"tifffile\"),\n",
    "    (\"scipy\", \"scipy\"),\n",
    "    (\"scikit-image\", \"skimage\"),  # pip: scikit-image, import: skimage\n",
    "    (\"pillow\", \"PIL\"),  # pip: pillow, import: PIL\n",
    "    (\"numpy\", \"numpy\"),\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Checking and installing required packages...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "installed_count = 0\n",
    "for pip_name, import_name in required_packages:\n",
    "    if install_package(pip_name, import_name):\n",
    "        installed_count += 1\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Package installation complete: {installed_count}/{len(required_packages)} packages ready\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâš ï¸  Note: If packages were just installed, you may need to restart the kernel\")\n",
    "print(\"   and run the cells again for imports to work properly.\")\n",
    "print(\"   In Jupyter: Kernel -> Restart & Run All\")\n",
    "print(\"   In Kaggle/Colab: Runtime -> Restart runtime\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root directory: data\n",
      "(Set DATA_ROOT environment variable to change, or edit DATA_ROOT in this cell)\n",
      "Warning: scikit-image not installed. Some metrics may not work. Install with: pip install scikit-image\n",
      "============================================================\n",
      "Dependency Status:\n",
      "============================================================\n",
      "Device: cpu\n",
      "tifffile: âœ… Available\n",
      "scipy: âœ… Available\n",
      "scikit-image: âŒ Not installed (required for topology metrics)\n",
      "============================================================\n",
      "\n",
      "âš ï¸  Warning: Missing dependencies: scikit-image (for topology metrics)\n",
      "   Install with: pip install scikit-image (for topology metrics)\n",
      "   Some features may not work correctly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import ImageFile, Image\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "# ============================================================================\n",
    "# Data Directory Configuration\n",
    "# ============================================================================\n",
    "# Set base data directory - can be overridden via environment variable\n",
    "# This allows the notebook to work on multiple PCs and cloud environments\n",
    "DATA_ROOT = os.environ.get('DATA_ROOT', 'data')  # Default: 'data' in current directory\n",
    "# Alternative: Set manually for your environment:\n",
    "# DATA_ROOT = '/kaggle/input/vesuvius-challenge-surface-detection'  # Kaggle\n",
    "# DATA_ROOT = '/content/data'  # Google Colab\n",
    "# DATA_ROOT = './data'  # Local development\n",
    "\n",
    "print(f\"Data root directory: {DATA_ROOT}\")\n",
    "print(f\"(Set DATA_ROOT environment variable to change, or edit DATA_ROOT in this cell)\")\n",
    "\n",
    "# Scientific computing imports (for metrics)\n",
    "try:\n",
    "    from scipy import ndimage\n",
    "    from scipy.ndimage import distance_transform_edt, binary_erosion, zoom\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: scipy not installed. Some metrics may not work. Install with: pip install scipy\")\n",
    "    SCIPY_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from skimage import measure, morphology\n",
    "    SKIMAGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: scikit-image not installed. Some metrics may not work. Install with: pip install scikit-image\")\n",
    "    SKIMAGE_AVAILABLE = False\n",
    "\n",
    "# Vesuvius Challenge specific imports\n",
    "try:\n",
    "    import tifffile\n",
    "    TIF_FILE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: tifffile not installed. Install with: pip install tifffile\")\n",
    "    TIF_FILE_AVAILABLE = False\n",
    "\n",
    "# Allow loading truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Suppress PIL warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='PIL')\n",
    "\n",
    "# Kaggle/Linux compatibility - use 2 workers on Linux, 0 on Windows\n",
    "NUM_WORKERS = 0 if sys.platform == 'win32' else 2\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print dependency status\n",
    "print(\"=\"*60)\n",
    "print(\"Dependency Status:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"tifffile: {'âœ… Available' if TIF_FILE_AVAILABLE else 'âŒ Not installed (required for Vesuvius Challenge)'}\")\n",
    "print(f\"scipy: {'âœ… Available' if SCIPY_AVAILABLE else 'âŒ Not installed (required for metrics and resizing)'}\")\n",
    "print(f\"scikit-image: {'âœ… Available' if SKIMAGE_AVAILABLE else 'âŒ Not installed (required for topology metrics)'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Warn if critical dependencies missing\n",
    "missing_deps = []\n",
    "if not TIF_FILE_AVAILABLE:\n",
    "    missing_deps.append(\"tifffile (for Vesuvius Challenge)\")\n",
    "if not SCIPY_AVAILABLE:\n",
    "    missing_deps.append(\"scipy (for metrics and resizing)\")\n",
    "if not SKIMAGE_AVAILABLE:\n",
    "    missing_deps.append(\"scikit-image (for topology metrics)\")\n",
    "\n",
    "if missing_deps:\n",
    "    print(f\"\\nâš ï¸  Warning: Missing dependencies: {', '.join(missing_deps)}\")\n",
    "    print(\"   Install with: pip install \" + \" \".join(missing_deps))\n",
    "    print(\"   Some features may not work correctly.\\n\")\n",
    "else:\n",
    "    print(\"\\nâœ… All dependencies available!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Classifier\n",
    "class ResNetClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-based classifier for medical image classification.\n",
    "    Uses a pretrained ResNet18 as backbone with a custom classification head.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=4, pretrained=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of output classes\n",
    "            pretrained: Whether to use pretrained ResNet weights\n",
    "        \"\"\"\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet18\n",
    "        if pretrained:\n",
    "            resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            resnet = models.resnet18(weights=None)\n",
    "        \n",
    "        # Remove the final fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # Get the number of features from ResNet18\n",
    "        num_features = resnet.fc.in_features\n",
    "        \n",
    "        # Custom classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet Segmentation Model\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # Pad if needed\n",
    "        diff_y = x2.size(2) - x1.size(2)\n",
    "        diff_x = x2.size(3) - x1.size(3)\n",
    "        x1 = F.pad(\n",
    "            x1,\n",
    "            [diff_x // 2, diff_x - diff_x // 2,\n",
    "             diff_y // 2, diff_y - diff_y // 2],\n",
    "        )\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class UNet2_5D(nn.Module):\n",
    "    \"\"\"2.5D UNet for Vesuvius Challenge - processes multi-slice context\"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input channels (slice_window for 2.5D)\n",
    "            out_channels: Number of output channels (1 for binary segmentation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Optional: Add attention mechanism for slice importance\n",
    "        self.slice_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W] where C = slice_window\n",
    "        # Apply slice attention if multi-channel\n",
    "        if x.shape[1] > 1:\n",
    "            attn = self.slice_attention(x)\n",
    "            x = x * attn\n",
    "        \n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Simple image/mask dataset for segmentation.\n",
    "    Expects matching filenames in images_dir and masks_dir.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images_dir, masks_dir, input_size=(256, 256)):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.input_size = input_size\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(images_dir)\n",
    "            if os.path.isfile(os.path.join(images_dir, f))\n",
    "        ])\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize(self.input_size),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize(self.input_size),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, fname)\n",
    "        mask_path = os.path.join(self.masks_dir, fname)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        image = self.img_transform(image)\n",
    "        mask = self.mask_transform(mask)\n",
    "        # binarize mask\n",
    "        mask = (mask > 0.5).float()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class VesuviusVolumeDataset(Dataset):\n",
    "    \"\"\"Dataset for 3D CT volumes from Vesuvius Challenge.\n",
    "    Handles variable dimensions and .tif volume files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        volume_dir: str,\n",
    "        mask_dir: Optional[str] = None,\n",
    "        mode: str = \"train\",\n",
    "        slice_window: int = 3,  # 2.5D: use N slices as context\n",
    "        target_size: Optional[Tuple[int, int]] = None,\n",
    "        augment: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            volume_dir: Directory containing .tif volume files\n",
    "            mask_dir: Directory containing .tif mask files (optional for test)\n",
    "            mode: 'train', 'val', or 'test'\n",
    "            slice_window: Number of slices to use as context (for 2.5D)\n",
    "            target_size: Optional (H, W) to resize to, None keeps original size\n",
    "            augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.volume_dir = Path(volume_dir)\n",
    "        self.mask_dir = Path(mask_dir) if mask_dir else None\n",
    "        self.mode = mode\n",
    "        self.slice_window = slice_window\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Find all volume files\n",
    "        self.volume_files = sorted(self.volume_dir.glob(\"*.tif\"))\n",
    "        if not self.volume_files:\n",
    "            self.volume_files = sorted(self.volume_dir.glob(\"*.tiff\"))\n",
    "        \n",
    "        if not self.volume_files:\n",
    "            raise ValueError(f\"No .tif/.tiff files found in {volume_dir}\")\n",
    "        \n",
    "        print(f\"Found {len(self.volume_files)} volume files\")\n",
    "        \n",
    "        # Pre-load volume metadata\n",
    "        self.volumes_info = []\n",
    "        for vol_path in self.volume_files:\n",
    "            if TIF_FILE_AVAILABLE:\n",
    "                try:\n",
    "                    # Just read shape without loading full volume\n",
    "                    with tifffile.TiffFile(vol_path) as tif:\n",
    "                        shape = tif.series[0].shape\n",
    "                        self.volumes_info.append({\n",
    "                            'path': vol_path,\n",
    "                            'shape': shape,\n",
    "                            'num_slices': shape[0] if len(shape) == 3 else 1\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not read {vol_path}: {e}\")\n",
    "            else:\n",
    "                # Fallback: assume standard shape\n",
    "                self.volumes_info.append({\n",
    "                    'path': vol_path,\n",
    "                    'shape': None,\n",
    "                    'num_slices': 100  # Default estimate\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of slices across all volumes\"\"\"\n",
    "        return sum(info['num_slices'] for info in self.volumes_info)\n",
    "    \n",
    "    def _get_volume_and_slice_idx(self, idx):\n",
    "        \"\"\"Convert global index to (volume_idx, slice_idx)\"\"\"\n",
    "        current = 0\n",
    "        for vol_idx, info in enumerate(self.volumes_info):\n",
    "            if idx < current + info['num_slices']:\n",
    "                slice_idx = idx - current\n",
    "                return vol_idx, slice_idx\n",
    "            current += info['num_slices']\n",
    "        # Fallback to last volume\n",
    "        return len(self.volumes_info) - 1, self.volumes_info[-1]['num_slices'] - 1\n",
    "    \n",
    "    def _load_slice_with_context(self, volume_path: Path, slice_idx: int) -> np.ndarray:\n",
    "        \"\"\"Load a slice with surrounding context slices (2.5D)\"\"\"\n",
    "        if not TIF_FILE_AVAILABLE:\n",
    "            raise ImportError(\"tifffile required for volume loading\")\n",
    "        \n",
    "        volume = tifffile.imread(str(volume_path))\n",
    "        num_slices = volume.shape[0]\n",
    "        \n",
    "        # Get slice range with padding\n",
    "        half_window = self.slice_window // 2\n",
    "        start_idx = max(0, slice_idx - half_window)\n",
    "        end_idx = min(num_slices, slice_idx + half_window + 1)\n",
    "        \n",
    "        # Extract slices\n",
    "        slices = volume[start_idx:end_idx]\n",
    "        \n",
    "        # Pad if needed\n",
    "        if len(slices) < self.slice_window:\n",
    "            padding = self.slice_window - len(slices)\n",
    "            if start_idx == 0:\n",
    "                # Pad at beginning\n",
    "                slices = np.concatenate([slices[:1].repeat(padding, axis=0), slices], axis=0)\n",
    "            else:\n",
    "                # Pad at end\n",
    "                slices = np.concatenate([slices, slices[-1:].repeat(padding, axis=0)], axis=0)\n",
    "        \n",
    "        # Stack slices: [C, H, W] where C = slice_window\n",
    "        if len(slices.shape) == 2:\n",
    "            slices = slices[np.newaxis, :]\n",
    "        \n",
    "        # Take center slice if we have more than needed\n",
    "        if slices.shape[0] > self.slice_window:\n",
    "            center = slices.shape[0] // 2\n",
    "            start = center - half_window\n",
    "            slices = slices[start:start + self.slice_window]\n",
    "        \n",
    "        # Combine slices into channels: [H, W, C] -> [C, H, W]\n",
    "        if slices.shape[0] == 1:\n",
    "            slice_data = slices[0]\n",
    "        else:\n",
    "            slice_data = np.transpose(slices, (1, 2, 0))  # [H, W, C]\n",
    "            slice_data = np.transpose(slice_data, (2, 0, 1))  # [C, H, W]\n",
    "        \n",
    "        return slice_data.astype(np.float32) / 255.0\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vol_idx, slice_idx = self._get_volume_and_slice_idx(idx)\n",
    "        vol_info = self.volumes_info[vol_idx]\n",
    "        \n",
    "        # Load slice with context\n",
    "        slice_data = self._load_slice_with_context(vol_info['path'], slice_idx)\n",
    "        \n",
    "        # Load corresponding mask if available\n",
    "        mask = None\n",
    "        if self.mask_dir and self.mode != \"test\":\n",
    "            mask_path = self.mask_dir / vol_info['path'].name\n",
    "            if mask_path.exists():\n",
    "                try:\n",
    "                    mask_volume = tifffile.imread(str(mask_path))\n",
    "                    mask = mask_volume[slice_idx].astype(np.float32)\n",
    "                    mask = (mask > 0.5).astype(np.float32)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load mask {mask_path}: {e}\")\n",
    "                    mask = np.zeros_like(slice_data[0] if len(slice_data.shape) == 3 else slice_data)\n",
    "        \n",
    "        # Resize if target_size specified\n",
    "        if self.target_size:\n",
    "            if not SCIPY_AVAILABLE:\n",
    "                raise ImportError(\"scipy required for resizing. Install with: pip install scipy\")\n",
    "            h, w = self.target_size\n",
    "            if len(slice_data.shape) == 3:\n",
    "                # Multi-channel\n",
    "                current_h, current_w = slice_data.shape[1], slice_data.shape[2]\n",
    "                zoom_factors = (1, h / current_h, w / current_w)\n",
    "                slice_data = zoom(slice_data, zoom_factors, order=1)\n",
    "                if mask is not None:\n",
    "                    mask = zoom(mask, (h / current_h, w / current_w), order=0)\n",
    "            else:\n",
    "                # Single channel\n",
    "                current_h, current_w = slice_data.shape\n",
    "                slice_data = zoom(slice_data, (h / current_h, w / current_w), order=1)\n",
    "                if mask is not None:\n",
    "                    mask = zoom(mask, (h / current_h, w / current_w), order=0)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        slice_tensor = torch.from_numpy(slice_data).float()\n",
    "        if mask is not None:\n",
    "            mask_tensor = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "        else:\n",
    "            mask_tensor = torch.zeros(slice_tensor.shape[-2:]).float().unsqueeze(0)\n",
    "        \n",
    "        return {\n",
    "            'image': slice_tensor,\n",
    "            'mask': mask_tensor,\n",
    "            'volume_id': vol_info['path'].stem,\n",
    "            'slice_idx': slice_idx\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_logits(logits, targets, eps=1e-6):\n",
    "    \"\"\"Dice loss for binary segmentation given logits.\"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    num = 2 * (probs * targets).sum(dim=(2, 3))\n",
    "    den = probs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3)) + eps\n",
    "    return 1 - (num / den).mean()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Vesuvius Challenge Competition Metrics\n",
    "# ============================================================================\n",
    "\n",
    "def surface_dice_score(pred: np.ndarray, target: np.ndarray, tolerance: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Surface Dice Score - measures distance between surfaces.\n",
    "    \n",
    "    Args:\n",
    "        pred: Binary prediction mask [H, W] or [Z, H, W]\n",
    "        target: Binary target mask [H, W] or [Z, H, W]\n",
    "        tolerance: Distance tolerance in pixels\n",
    "    \n",
    "    Returns:\n",
    "        Surface Dice score (0-1, higher is better)\n",
    "    \"\"\"\n",
    "    pred = (pred > 0.5).astype(np.uint8)\n",
    "    target = (target > 0.5).astype(np.uint8)\n",
    "    \n",
    "    if pred.ndim == 3:\n",
    "        # 3D volume: compute per-slice and average\n",
    "        scores = []\n",
    "        for z in range(pred.shape[0]):\n",
    "            if target[z].sum() > 0 or pred[z].sum() > 0:\n",
    "                score = surface_dice_score(pred[z], target[z], tolerance)\n",
    "                scores.append(score)\n",
    "        return np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    # 2D: compute surface distances\n",
    "    if not SCIPY_AVAILABLE:\n",
    "        # Fallback to simple Dice if scipy not available\n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum()\n",
    "        return float(2 * intersection / union) if union > 0 else 0.0\n",
    "    \n",
    "    # Get surfaces (boundaries)\n",
    "    pred_surface = pred - binary_erosion(pred)\n",
    "    target_surface = target - binary_erosion(target)\n",
    "    \n",
    "    if pred_surface.sum() == 0 and target_surface.sum() == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # Distance from pred surface to target surface\n",
    "    dist_pred_to_target = distance_transform_edt(~target_surface)\n",
    "    dist_target_to_pred = distance_transform_edt(~pred_surface)\n",
    "    \n",
    "    # Count surface points within tolerance\n",
    "    pred_in_tolerance = (dist_pred_to_target[pred_surface > 0] <= tolerance).sum()\n",
    "    target_in_tolerance = (dist_target_to_pred[target_surface > 0] <= tolerance).sum()\n",
    "    \n",
    "    total_pred_surface = pred_surface.sum()\n",
    "    total_target_surface = target_surface.sum()\n",
    "    \n",
    "    if total_pred_surface == 0 or total_target_surface == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    surface_dice = (pred_in_tolerance + target_in_tolerance) / (total_pred_surface + total_target_surface)\n",
    "    return float(surface_dice)\n",
    "\n",
    "\n",
    "def topo_score(pred: np.ndarray, target: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    TopoScore - measures topological similarity.\n",
    "    Penalizes artificial mergers and splits.\n",
    "    \n",
    "    Args:\n",
    "        pred: Binary prediction mask [H, W] or [Z, H, W]\n",
    "        target: Binary target mask [H, W] or [Z, H, W]\n",
    "    \n",
    "    Returns:\n",
    "        TopoScore (0-1, higher is better)\n",
    "    \"\"\"\n",
    "    pred = (pred > 0.5).astype(np.uint8)\n",
    "    target = (target > 0.5).astype(np.uint8)\n",
    "    \n",
    "    if pred.ndim == 3:\n",
    "        # 3D: compute per-slice and average\n",
    "        scores = []\n",
    "        for z in range(pred.shape[0]):\n",
    "            if target[z].sum() > 0 or pred[z].sum() > 0:\n",
    "                score = topo_score(pred[z], target[z])\n",
    "                scores.append(score)\n",
    "        return np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    # 2D: count connected components\n",
    "    if not SKIMAGE_AVAILABLE:\n",
    "        # Fallback: simple component counting\n",
    "        return 1.0 if (pred == target).all() else 0.5\n",
    "    \n",
    "    pred_labels = measure.label(pred)\n",
    "    target_labels = measure.label(target)\n",
    "    \n",
    "    num_pred_components = pred_labels.max()\n",
    "    num_target_components = target_labels.max()\n",
    "    \n",
    "    if num_target_components == 0:\n",
    "        return 1.0 if num_pred_components == 0 else 0.0\n",
    "    \n",
    "    # Penalize difference in number of components\n",
    "    component_ratio = min(num_pred_components, num_target_components) / max(num_pred_components, num_target_components)\n",
    "    \n",
    "    return float(component_ratio)\n",
    "\n",
    "\n",
    "def voi_score(pred: np.ndarray, target: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Variation of Information (VOI) - measures information distance.\n",
    "    \n",
    "    Args:\n",
    "        pred: Binary prediction mask [H, W] or [Z, H, W]\n",
    "        target: Binary target mask [H, W] or [Z, H, W]\n",
    "    \n",
    "    Returns:\n",
    "        Normalized VOI score (0-1, higher is better, lower VOI is better)\n",
    "    \"\"\"\n",
    "    pred = (pred > 0.5).astype(np.uint8)\n",
    "    target = (target > 0.5).astype(np.uint8)\n",
    "    \n",
    "    if pred.ndim == 3:\n",
    "        # 3D: compute per-slice and average\n",
    "        scores = []\n",
    "        for z in range(pred.shape[0]):\n",
    "            if target[z].sum() > 0 or pred[z].sum() > 0:\n",
    "                score = voi_score(pred[z], target[z])\n",
    "                scores.append(score)\n",
    "        return np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    # 2D: compute VOI\n",
    "    if not SKIMAGE_AVAILABLE:\n",
    "        # Fallback: simple similarity\n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum()\n",
    "        return float(intersection / union) if union > 0 else 0.0\n",
    "    \n",
    "    pred_labels = measure.label(pred)\n",
    "    target_labels = measure.label(target)\n",
    "    \n",
    "    # Get unique labels\n",
    "    pred_unique = np.unique(pred_labels)\n",
    "    target_unique = np.unique(target_labels)\n",
    "    \n",
    "    # Remove background (0)\n",
    "    pred_unique = pred_unique[pred_unique > 0]\n",
    "    target_unique = target_unique[target_unique > 0]\n",
    "    \n",
    "    if len(pred_unique) == 0 and len(target_unique) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # Compute joint histogram\n",
    "    h, w = pred.shape\n",
    "    total_pixels = h * w\n",
    "    \n",
    "    # Compute entropy\n",
    "    pred_entropy = 0.0\n",
    "    for label in pred_unique:\n",
    "        p = (pred_labels == label).sum() / total_pixels\n",
    "        if p > 0:\n",
    "            pred_entropy -= p * np.log2(p)\n",
    "    \n",
    "    target_entropy = 0.0\n",
    "    for label in target_unique:\n",
    "        p = (target_labels == label).sum() / total_pixels\n",
    "        if p > 0:\n",
    "            target_entropy -= p * np.log2(p)\n",
    "    \n",
    "    # Compute mutual information\n",
    "    mi = 0.0\n",
    "    for p_label in pred_unique:\n",
    "        for t_label in target_unique:\n",
    "            joint = ((pred_labels == p_label) & (target_labels == t_label)).sum() / total_pixels\n",
    "            p_pred = (pred_labels == p_label).sum() / total_pixels\n",
    "            p_target = (target_labels == t_label).sum() / total_pixels\n",
    "            if joint > 0 and p_pred > 0 and p_target > 0:\n",
    "                mi += joint * np.log2(joint / (p_pred * p_target))\n",
    "    \n",
    "    # VOI = H(pred) + H(target) - 2*MI\n",
    "    voi = pred_entropy + target_entropy - 2 * mi\n",
    "    \n",
    "    # Normalize to [0, 1] (higher is better)\n",
    "    max_entropy = max(pred_entropy, target_entropy) * 2\n",
    "    normalized_voi = 1.0 - (voi / max_entropy) if max_entropy > 0 else 1.0\n",
    "    \n",
    "    return float(np.clip(normalized_voi, 0.0, 1.0))\n",
    "\n",
    "\n",
    "def compute_competition_metrics(pred: np.ndarray, target: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Compute all Vesuvius Challenge competition metrics.\n",
    "    \n",
    "    Args:\n",
    "        pred: Prediction mask [H, W] or [Z, H, W]\n",
    "        target: Target mask [H, W] or [Z, H, W]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics: surface_dice, topo_score, voi_score, combined_score\n",
    "    \"\"\"\n",
    "    surface_dice = surface_dice_score(pred, target)\n",
    "    topo = topo_score(pred, target)\n",
    "    voi = voi_score(pred, target)\n",
    "    \n",
    "    # Combined score (weighted average as per competition)\n",
    "    # Weights may vary - using equal weights as default\n",
    "    combined = (surface_dice * 0.4 + topo * 0.3 + voi * 0.3)\n",
    "    \n",
    "    return {\n",
    "        'surface_dice': surface_dice,\n",
    "        'topo_score': topo,\n",
    "        'voi_score': voi,\n",
    "        'combined_score': combined\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded for domains: ['brain', 'lungs', 'skin', 'breast', 'bone']\n",
      "Data root: data\n"
     ]
    }
   ],
   "source": [
    "# Classification configuration per domain\n",
    "# All paths are relative to DATA_ROOT (set above)\n",
    "CONFIG = {\n",
    "    \"brain\": {\n",
    "        \"train_path\": os.path.join(DATA_ROOT, \"brain/brain/Training\"),\n",
    "        \"val_path\": os.path.join(DATA_ROOT, \"brain/brain/Testing\"),\n",
    "        \"test_path\": os.path.join(DATA_ROOT, \"brain/brain/Testing\"),  # Using Testing as test set\n",
    "        \"model\": ResNetClassifier(num_classes=4),\n",
    "        \"input_size\": (224, 224),\n",
    "    },\n",
    "    \"lungs\": {\n",
    "        \"train_path\": os.path.join(DATA_ROOT, \"lungs/train\"),\n",
    "        \"val_path\": os.path.join(DATA_ROOT, \"lungs/val\"),\n",
    "        \"test_path\": os.path.join(DATA_ROOT, \"lungs/val\"),  # Using val as test if no separate test set\n",
    "        \"model\": ResNetClassifier(num_classes=5),\n",
    "        \"input_size\": (224, 224),\n",
    "    },\n",
    "    \"skin\": {\n",
    "        \"train_path\": os.path.join(DATA_ROOT, \"skin/train\"),\n",
    "        \"val_path\": os.path.join(DATA_ROOT, \"skin/val\"),\n",
    "        \"test_path\": os.path.join(DATA_ROOT, \"skin/val\"),  # Using val as test if no separate test set\n",
    "        # Skin dataset has 9 classes (including Vascular lesion)\n",
    "        \"model\": ResNetClassifier(num_classes=9),\n",
    "        \"input_size\": (128, 128),\n",
    "    },\n",
    "    \"breast\": {\n",
    "        \"train_path\": os.path.join(DATA_ROOT, \"breast_split/train\") if os.path.exists(os.path.join(DATA_ROOT, \"breast_split/train\")) else os.path.join(DATA_ROOT, \"breast\"),\n",
    "        \"val_path\": os.path.join(DATA_ROOT, \"breast_split/val\") if os.path.exists(os.path.join(DATA_ROOT, \"breast_split/val\")) else os.path.join(DATA_ROOT, \"breast\"),\n",
    "        \"test_path\": os.path.join(DATA_ROOT, \"breast_split/test\") if os.path.exists(os.path.join(DATA_ROOT, \"breast_split/test\")) else os.path.join(DATA_ROOT, \"breast\"),\n",
    "        \"model\": ResNetClassifier(num_classes=3),\n",
    "        \"input_size\": (128, 128),\n",
    "    },\n",
    "    \"bone\": {\n",
    "        \"train_path\": os.path.join(DATA_ROOT, \"Bone/train\"),\n",
    "        \"val_path\": os.path.join(DATA_ROOT, \"Bone/val\"),\n",
    "        \"test_path\": os.path.join(DATA_ROOT, \"Bone/test\"),\n",
    "        \"model\": ResNetClassifier(num_classes=2),\n",
    "        \"input_size\": (224, 224),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded for domains:\", list(CONFIG.keys()))\n",
    "print(f\"Data root: {DATA_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(name, config):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training on: {name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Use simple torchvision transforms for training (with augmentation)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(config[\"input_size\"]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Use simple transforms for validation and testing (no augmentation)\n",
    "    eval_transform = transforms.Compose([\n",
    "        transforms.Resize(config[\"input_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Load datasets\n",
    "    train_dir = config[\"train_path\"]\n",
    "    val_dir = config[\"val_path\"]\n",
    "    test_dir = config.get(\"test_path\", val_dir)  # Use test_path if available, else val\n",
    "\n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(train_dir):\n",
    "        print(f\"[ERROR] Training directory not found: {train_dir}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(val_dir):\n",
    "        print(f\"[WARNING] Validation directory not found: {val_dir}, using train directory\")\n",
    "        val_dir = train_dir\n",
    "    \n",
    "    if not os.path.exists(test_dir):\n",
    "        print(f\"[WARNING] Test directory not found: {test_dir}, using validation directory\")\n",
    "        test_dir = val_dir\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "    val_dataset = datasets.ImageFolder(val_dir, transform=eval_transform)\n",
    "    test_dataset = datasets.ImageFolder(test_dir, transform=eval_transform)\n",
    "    \n",
    "    print(f\"\\nDataset Information:\")\n",
    "    print(f\"   Training samples: {len(train_dataset)}\")\n",
    "    print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"   Testing samples: {len(test_dataset)}\")\n",
    "    print(f\"   Classes: {train_dataset.classes}\")\n",
    "    print(f\"   Number of classes: {len(train_dataset.classes)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              pin_memory=True if torch.cuda.is_available() else False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=True if torch.cuda.is_available() else False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             pin_memory=True if torch.cuda.is_available() else False)\n",
    "\n",
    "    # Model setup\n",
    "    model = config[\"model\"].to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Batch size: 32\\n\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress every 50 batches\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                current_acc = 100. * correct / total\n",
    "                print(f\"  Batch {batch_idx + 1}/{len(train_loader)}: Loss={running_loss/num_batches:.4f}, Acc={current_acc:.2f}%\")\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_acc = 100. * val_correct / val_total if val_total > 0 else 0.0\n",
    "        print(f\"Epoch {epoch}: Train Loss={running_loss:.4f} | Train Acc={train_acc:.2f}% | Val Loss={val_loss:.4f} | Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final Test Evaluation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    model.eval()\n",
    "    test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_acc = 100. * test_correct / test_total if test_total > 0 else 0.0\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"   Correct: {test_correct}/{test_total}\")\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "    for pred, label in zip(all_preds, all_labels):\n",
    "        class_total[label] += 1\n",
    "        if pred == label:\n",
    "            class_correct[label] += 1\n",
    "    \n",
    "    print(f\"\\nPer-Class Accuracy:\")\n",
    "    for i, class_name in enumerate(test_dataset.classes):\n",
    "        if class_total[i] > 0:\n",
    "            acc = 100. * class_correct[i] / class_total[i]\n",
    "            print(f\"   {class_name}: {acc:.2f}% ({class_correct[i]}/{class_total[i]})\")\n",
    "\n",
    "    # Save model weights into central 'trained' folder\n",
    "    os.makedirs(\"trained\", exist_ok=True)\n",
    "    save_path = os.path.join(\"trained\", f\"{name.lower()}_model.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"\\n[SUCCESS] Model saved: {save_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Vesuvius Challenge: Volume Output Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_volume(model, volume_path: str, output_path: str, \n",
    "                   slice_window: int = 3, device: str = \"cuda\",\n",
    "                   batch_size: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate predictions for an entire 3D volume and save as .tif.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained segmentation model\n",
    "        volume_path: Path to input .tif volume\n",
    "        output_path: Path to save output .tif mask\n",
    "        slice_window: Number of slices for 2.5D context\n",
    "        device: Device to run inference on\n",
    "        batch_size: Batch size for inference\n",
    "    \n",
    "    Returns:\n",
    "        Predicted volume mask as numpy array\n",
    "    \"\"\"\n",
    "    if not TIF_FILE_AVAILABLE:\n",
    "        raise ImportError(\"tifffile required for volume I/O\")\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load volume\n",
    "    print(f\"Loading volume: {volume_path}\")\n",
    "    volume = tifffile.imread(volume_path)\n",
    "    print(f\"Volume shape: {volume.shape}\")\n",
    "    \n",
    "    num_slices, height, width = volume.shape\n",
    "    predictions = np.zeros((num_slices, height, width), dtype=np.float32)\n",
    "    \n",
    "    # Process slices in batches\n",
    "    half_window = slice_window // 2\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for slice_idx in range(num_slices):\n",
    "            # Get slice range with context\n",
    "            start_idx = max(0, slice_idx - half_window)\n",
    "            end_idx = min(num_slices, slice_idx + half_window + 1)\n",
    "            \n",
    "            # Extract slices\n",
    "            slices = volume[start_idx:end_idx].astype(np.float32) / 255.0\n",
    "            \n",
    "            # Pad if needed\n",
    "            if len(slices) < slice_window:\n",
    "                padding = slice_window - len(slices)\n",
    "                if start_idx == 0:\n",
    "                    slices = np.concatenate([slices[:1].repeat(padding, axis=0), slices], axis=0)\n",
    "                else:\n",
    "                    slices = np.concatenate([slices, slices[-1:].repeat(padding, axis=0)], axis=0)\n",
    "            \n",
    "            # Take center slice if we have more than needed\n",
    "            if len(slices) > slice_window:\n",
    "                center = len(slices) // 2\n",
    "                start = center - half_window\n",
    "                slices = slices[start:start + slice_window]\n",
    "            \n",
    "            # Convert to tensor: [C, H, W]\n",
    "            if len(slices.shape) == 2:\n",
    "                slice_tensor = torch.from_numpy(slices).float().unsqueeze(0)\n",
    "            else:\n",
    "                slice_tensor = torch.from_numpy(slices).float()\n",
    "                if slice_tensor.shape[0] != slice_window:\n",
    "                    # Reshape if needed\n",
    "                    slice_tensor = slice_tensor.transpose(1, 2).transpose(0, 1)\n",
    "            \n",
    "            slice_tensor = slice_tensor.unsqueeze(0).to(device)  # [1, C, H, W]\n",
    "            \n",
    "            # Predict\n",
    "            logits = model(slice_tensor)\n",
    "            pred = torch.sigmoid(logits).cpu().numpy()[0, 0]  # [H, W]\n",
    "            \n",
    "            predictions[slice_idx] = pred\n",
    "            \n",
    "            if (slice_idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {slice_idx + 1}/{num_slices} slices\")\n",
    "    \n",
    "    # Binarize predictions\n",
    "    predictions_binary = (predictions > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    # Save as .tif\n",
    "    print(f\"Saving predictions to: {output_path}\")\n",
    "    tifffile.imwrite(output_path, predictions_binary)\n",
    "    \n",
    "    return predictions_binary\n",
    "\n",
    "\n",
    "def generate_submission(model, test_volume_dir: str, output_dir: str,\n",
    "                       slice_window: int = 3, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate submission files for all test volumes.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained segmentation model\n",
    "        test_volume_dir: Directory containing test .tif volumes\n",
    "        output_dir: Directory to save submission .tif files\n",
    "        slice_window: Number of slices for 2.5D context\n",
    "        device: Device to run inference on\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    volume_dir = Path(test_volume_dir)\n",
    "    volume_files = sorted(volume_dir.glob(\"*.tif\"))\n",
    "    if not volume_files:\n",
    "        volume_files = sorted(volume_dir.glob(\"*.tiff\"))\n",
    "    \n",
    "    print(f\"Found {len(volume_files)} test volumes\")\n",
    "    \n",
    "    for vol_path in volume_files:\n",
    "        image_id = vol_path.stem\n",
    "        output_path = Path(output_dir) / f\"{image_id}.tif\"\n",
    "        \n",
    "        print(f\"\\nProcessing {image_id}...\")\n",
    "        try:\n",
    "            predict_volume(model, str(vol_path), str(output_path), \n",
    "                          slice_window=slice_window, device=device)\n",
    "            print(f\"âœ… Saved: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {image_id}: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Submission files saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vesuvius Challenge Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vesuvius_challenge(\n",
    "    volume_dir: str,\n",
    "    mask_dir: str,\n",
    "    output_dir: str = \"trained\",\n",
    "    slice_window: int = 3,\n",
    "    epochs: int = 20,\n",
    "    batch_size: int = 4,\n",
    "    learning_rate: float = 1e-4,\n",
    "    target_size: Optional[Tuple[int, int]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train model for Vesuvius Challenge 3D volume segmentation.\n",
    "    \n",
    "    Args:\n",
    "        volume_dir: Directory containing training .tif volumes\n",
    "        mask_dir: Directory containing training .tif masks\n",
    "        output_dir: Directory to save trained model\n",
    "        slice_window: Number of slices for 2.5D context (3, 5, or 7)\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        target_size: Optional (H, W) to resize to, None keeps original size\n",
    "    \"\"\"\n",
    "    if not TIF_FILE_AVAILABLE:\n",
    "        raise ImportError(\"tifffile required. Install with: pip install tifffile\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Vesuvius Challenge Training\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Volume directory: {volume_dir}\")\n",
    "    print(f\"Mask directory: {mask_dir}\")\n",
    "    print(f\"Slice window: {slice_window}\")\n",
    "    print(f\"Target size: {target_size}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = VesuviusVolumeDataset(\n",
    "        volume_dir=volume_dir,\n",
    "        mask_dir=mask_dir,\n",
    "        mode=\"train\",\n",
    "        slice_window=slice_window,\n",
    "        target_size=target_size,\n",
    "        augment=True\n",
    "    )\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"[ERROR] No samples found in dataset\")\n",
    "        return\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = UNet2_5D(in_channels=slice_window, out_channels=1).to(DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    print(f\"\\nModel: UNet2.5D (in_channels={slice_window})\")\n",
    "    print(f\"Dataset size: {len(dataset)} samples\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    best_score = 0.0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            images = batch['image'].to(DEVICE)  # [B, C, H, W]\n",
    "            masks = batch['mask'].to(DEVICE)     # [B, 1, H, W]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            \n",
    "            # Combined loss: BCE + Dice\n",
    "            bce_loss = criterion(logits, masks)\n",
    "            dice_loss = dice_loss_logits(logits, masks)\n",
    "            loss = 0.5 * bce_loss + 0.5 * dice_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Store predictions for metrics\n",
    "            preds = torch.sigmoid(logits).cpu().numpy()\n",
    "            targets = masks.cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(targets)\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"  Batch {batch_idx + 1}/{len(loader)}: Loss={loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        \n",
    "        # Compute competition metrics\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "        \n",
    "        # Average metrics across batch\n",
    "        metrics_list = []\n",
    "        for i in range(len(all_preds)):\n",
    "            pred = all_preds[i, 0]  # [H, W]\n",
    "            target = all_targets[i, 0]  # [H, W]\n",
    "            metrics = compute_competition_metrics(pred, target)\n",
    "            metrics_list.append(metrics)\n",
    "        \n",
    "        avg_metrics = {\n",
    "            'surface_dice': np.mean([m['surface_dice'] for m in metrics_list]),\n",
    "            'topo_score': np.mean([m['topo_score'] for m in metrics_list]),\n",
    "            'voi_score': np.mean([m['voi_score'] for m in metrics_list]),\n",
    "            'combined_score': np.mean([m['combined_score'] for m in metrics_list])\n",
    "        }\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch}/{epochs}:\")\n",
    "        print(f\"  Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Surface Dice: {avg_metrics['surface_dice']:.4f}\")\n",
    "        print(f\"  TopoScore: {avg_metrics['topo_score']:.4f}\")\n",
    "        print(f\"  VOI Score: {avg_metrics['voi_score']:.4f}\")\n",
    "        print(f\"  Combined Score: {avg_metrics['combined_score']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_metrics['combined_score'] > best_score:\n",
    "            best_score = avg_metrics['combined_score']\n",
    "            model_path = os.path.join(output_dir, \"vesuvius_unet2.5d_best.pth\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"  [BEST] Saved model to {model_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training finished. Best combined score: {best_score:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Vesuvius Challenge Training\n",
    "\n",
    "Configure paths and run training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  WARNING: scikit-image is recommended for topology metrics\n",
      "   Install with: pip install scikit-image\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Vesuvius Challenge Configuration\n",
    "# ============================================================================\n",
    "# Update these paths to match your data directory structure\n",
    "# All paths are relative to DATA_ROOT (set in cell 2)\n",
    "\n",
    "VESUVIUS_VOLUME_DIR = os.path.join(DATA_ROOT, \"vesuvius/train/volumes\")      # Directory with .tif volumes\n",
    "VESUVIUS_MASK_DIR = os.path.join(DATA_ROOT, \"vesuvius/train/masks\")         # Directory with .tif masks\n",
    "VESUVIUS_SLICE_WINDOW = 3                                # 2.5D: use 3 slices (center + 1 on each side)\n",
    "                                                          # Options: 3, 5, or 7 (more slices = more context but slower)\n",
    "VESUVIUS_EPOCHS = 20\n",
    "VESUVIUS_BATCH_SIZE = 4                                  # Reduce if OOM errors occur\n",
    "VESUVIUS_TARGET_SIZE = None                              # None = keep original size, or (256, 256) to resize\n",
    "VESUVIUS_LEARNING_RATE = 1e-4\n",
    "VESUVIUS_OUTPUT_DIR = \"trained\"                          # Directory to save trained models\n",
    "\n",
    "# Check if dependencies are available\n",
    "if not TIF_FILE_AVAILABLE:\n",
    "    print(\"âŒ ERROR: tifffile is required for Vesuvius Challenge training!\")\n",
    "    print(\"   Install with: pip install tifffile\")\n",
    "elif not SCIPY_AVAILABLE:\n",
    "    print(\"âš ï¸  WARNING: scipy is recommended for resizing and metrics\")\n",
    "    print(\"   Install with: pip install scipy\")\n",
    "elif not SKIMAGE_AVAILABLE:\n",
    "    print(\"âš ï¸  WARNING: scikit-image is recommended for topology metrics\")\n",
    "    print(\"   Install with: pip install scikit-image\")\n",
    "else:\n",
    "    print(\"âœ… All dependencies available for Vesuvius Challenge training\")\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Volume directory: {VESUVIUS_VOLUME_DIR}\")\n",
    "    print(f\"  Mask directory: {VESUVIUS_MASK_DIR}\")\n",
    "    print(f\"  Slice window: {VESUVIUS_SLICE_WINDOW}\")\n",
    "    print(f\"  Epochs: {VESUVIUS_EPOCHS}\")\n",
    "    print(f\"  Batch size: {VESUVIUS_BATCH_SIZE}\")\n",
    "    print(f\"  Target size: {VESUVIUS_TARGET_SIZE}\")\n",
    "    print(f\"\\nTo start training, uncomment the code below:\\n\")\n",
    "\n",
    "# Uncomment to run Vesuvius Challenge training:\n",
    "# try:\n",
    "#     model = train_vesuvius_challenge(\n",
    "#         volume_dir=VESUVIUS_VOLUME_DIR,\n",
    "#         mask_dir=VESUVIUS_MASK_DIR,\n",
    "#         output_dir=VESUVIUS_OUTPUT_DIR,\n",
    "#         slice_window=VESUVIUS_SLICE_WINDOW,\n",
    "#         epochs=VESUVIUS_EPOCHS,\n",
    "#         batch_size=VESUVIUS_BATCH_SIZE,\n",
    "#         learning_rate=VESUVIUS_LEARNING_RATE,\n",
    "#         target_size=VESUVIUS_TARGET_SIZE\n",
    "#     )\n",
    "#     print(\"\\nâœ… Training completed successfully!\")\n",
    "# except FileNotFoundError as e:\n",
    "#     print(f\"\\nâŒ Error: Data directory not found: {e}\")\n",
    "#     print(\"   Please update VESUVIUS_VOLUME_DIR and VESUVIUS_MASK_DIR above\")\n",
    "# except ImportError as e:\n",
    "#     print(f\"\\nâŒ Error: Missing dependency: {e}\")\n",
    "#     print(\"   Install required packages: pip install tifffile scipy scikit-image\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nâŒ Error during training: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Submission Files\n",
    "\n",
    "After training, use this to generate .tif submission files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ERROR: Model file not found: trained/vesuvius_unet2.5d_best.pth\n",
      "   Please train a model first or update MODEL_PATH above\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Generate Submission Files for Vesuvius Challenge\n",
    "# ============================================================================\n",
    "# After training, use this cell to generate .tif submission files\n",
    "\n",
    "# Configuration\n",
    "# Paths are relative to current directory (for outputs) or DATA_ROOT (for inputs)\n",
    "MODEL_PATH = \"trained/vesuvius_unet2.5d_best.pth\"        # Path to trained model (relative to notebook)\n",
    "TEST_VOLUME_DIR = os.path.join(DATA_ROOT, \"vesuvius/test/volumes\")           # Directory with test .tif volumes\n",
    "SUBMISSION_DIR = \"submission\"                            # Directory to save submission files (relative to notebook)\n",
    "\n",
    "# Check dependencies\n",
    "if not TIF_FILE_AVAILABLE:\n",
    "    print(\"âŒ ERROR: tifffile is required for submission generation!\")\n",
    "    print(\"   Install with: pip install tifffile\")\n",
    "elif not os.path.exists(MODEL_PATH):\n",
    "    print(f\"âŒ ERROR: Model file not found: {MODEL_PATH}\")\n",
    "    print(\"   Please train a model first or update MODEL_PATH above\")\n",
    "elif not os.path.exists(TEST_VOLUME_DIR):\n",
    "    print(f\"âš ï¸  WARNING: Test volume directory not found: {TEST_VOLUME_DIR}\")\n",
    "    print(\"   Please update TEST_VOLUME_DIR above or create the directory\")\n",
    "else:\n",
    "    print(\"âœ… Ready to generate submission files\")\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Model: {MODEL_PATH}\")\n",
    "    print(f\"  Test volumes: {TEST_VOLUME_DIR}\")\n",
    "    print(f\"  Output directory: {SUBMISSION_DIR}\")\n",
    "    print(f\"  Slice window: {VESUVIUS_SLICE_WINDOW}\")\n",
    "    print(f\"\\nTo generate submission, uncomment the code below:\\n\")\n",
    "\n",
    "# Uncomment to generate submission files:\n",
    "# try:\n",
    "#     # Load model\n",
    "#     print(f\"Loading model from {MODEL_PATH}...\")\n",
    "#     model = UNet2_5D(in_channels=VESUVIUS_SLICE_WINDOW, out_channels=1)\n",
    "#     model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "#     model = model.to(DEVICE)\n",
    "#     model.eval()\n",
    "#     print(\"âœ… Model loaded successfully\")\n",
    "#     \n",
    "#     # Generate submission\n",
    "#     print(f\"\\nGenerating submission files...\")\n",
    "#     generate_submission(\n",
    "#         model=model,\n",
    "#         test_volume_dir=TEST_VOLUME_DIR,\n",
    "#         output_dir=SUBMISSION_DIR,\n",
    "#         slice_window=VESUVIUS_SLICE_WINDOW,\n",
    "#         device=str(DEVICE)\n",
    "#     )\n",
    "#     \n",
    "#     print(f\"\\nâœ… Submission files saved to: {SUBMISSION_DIR}\")\n",
    "#     print(\"   Zip the directory and submit to Kaggle!\")\n",
    "#     \n",
    "# except FileNotFoundError as e:\n",
    "#     print(f\"\\nâŒ Error: File not found: {e}\")\n",
    "#     print(\"   Please check MODEL_PATH and TEST_VOLUME_DIR\")\n",
    "# except RuntimeError as e:\n",
    "#     print(f\"\\nâŒ Error: Model loading failed: {e}\")\n",
    "#     print(\"   Make sure MODEL_PATH points to a valid trained model\")\n",
    "#     print(\"   Check that VESUVIUS_SLICE_WINDOW matches training configuration\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nâŒ Error during submission generation: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "### For Medical Image Classification:\n",
    "1. Run cells 1-6 (imports, models, config)\n",
    "2. Set `TASK = \"classification\"` and `DOMAIN` in cell 8\n",
    "3. Run cell 8 to train\n",
    "\n",
    "### For Vesuvius Challenge:\n",
    "1. **Install dependencies**: `pip install tifffile scipy scikit-image`\n",
    "2. Run cells 1-6 (imports, models)\n",
    "3. Update paths in cell 11 (Vesuvius Challenge Configuration)\n",
    "4. Uncomment and run training code in cell 11\n",
    "5. After training, update paths in cell 12 (Submission Generation)\n",
    "6. Uncomment and run submission code in cell 12\n",
    "7. Zip the submission directory and upload to Kaggle\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Missing dependencies**: Check cell 2 for dependency status\n",
    "- **OOM errors**: Reduce `VESUVIUS_BATCH_SIZE` or `target_size`\n",
    "- **File not found**: Update directory paths in configuration cells\n",
    "- **Model loading errors**: Ensure `VESUVIUS_SLICE_WINDOW` matches training config\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "**Vesuvius Challenge Competition:**\n",
    "- [Kaggle Competition Page](https://kaggle.com/competitions/vesuvius-challenge-surface-detection)\n",
    "- [Official Website](https://scrollprize.org)\n",
    "- [Data Portal](https://dl.ash2txt.org)\n",
    "\n",
    "**Citation:**\n",
    "```bibtex\n",
    "@misc{vesuvius-challenge-surface-detection,\n",
    "    author = {Sean Johnson and David Josey and Elian Rafael Dal PrÃ  and Hendrik Schilling and Youssef Nader and Johannes Rudolph and Forrest McDonald and Paul Henderson and Giorgio Angelotti and Sohier Dane and MarÃ­a Cruz},\n",
    "    title = {Vesuvius Challenge - Surface Detection},\n",
    "    year = {2025},\n",
    "    howpublished = {\\url{https://kaggle.com/competitions/vesuvius-challenge-surface-detection}},\n",
    "    note = {Kaggle}\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segmentation(images_dir, masks_dir, output_path,\n",
    "                       input_size=(256, 256), epochs=20, batch_size=4):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training segmentation UNet\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Images dir: {images_dir}\")\n",
    "    print(f\"Masks dir:  {masks_dir}\")\n",
    "\n",
    "    if not os.path.isdir(images_dir):\n",
    "        print(f\"[ERROR] Images directory not found: {images_dir}\")\n",
    "        return\n",
    "    if not os.path.isdir(masks_dir):\n",
    "        print(f\"[ERROR] Masks directory not found: {masks_dir}\")\n",
    "        return\n",
    "\n",
    "    dataset = SegmentationDataset(images_dir, masks_dir, input_size=input_size)\n",
    "    if len(dataset) == 0:\n",
    "        print(\"[ERROR] No segmentation samples found.\")\n",
    "        return\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                        num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = UNet(in_channels=1, out_channels=1).to(DEVICE)\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    print(f\"Dataset size: {len(dataset)} samples\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for imgs, masks in loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imgs)\n",
    "            loss = 0.5 * bce(logits, masks) + 0.5 * dice_loss_logits(logits, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        print(f\"Epoch {epoch}/{epochs}: Segmentation loss={avg_loss:.4f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), output_path)\n",
    "            print(f\"  [BEST] Saved UNet to {output_path}\")\n",
    "\n",
    "    print(f\"\\nSegmentation training finished. Best loss: {best_loss:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Training\n",
    "\n",
    "Configure the task and domain below, then run this cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: classification\n",
      "Domain: all\n",
      "\n",
      "============================================================\n",
      "Training on: BRAIN\n",
      "============================================================\n",
      "[ERROR] Training directory not found: data\\brain/brain/Training\n",
      "\n",
      "============================================================\n",
      "Training on: LUNGS\n",
      "============================================================\n",
      "[ERROR] Training directory not found: data\\lungs/train\n",
      "\n",
      "============================================================\n",
      "Training on: SKIN\n",
      "============================================================\n",
      "[ERROR] Training directory not found: data\\skin/train\n",
      "\n",
      "============================================================\n",
      "Training on: BREAST\n",
      "============================================================\n",
      "[ERROR] Training directory not found: data\\breast\n",
      "\n",
      "============================================================\n",
      "Training on: BONE\n",
      "============================================================\n",
      "[ERROR] Training directory not found: data\\Bone/train\n"
     ]
    }
   ],
   "source": [
    "# Configuration: choose task and domain\n",
    "# Options:\n",
    "#   TASK: \"classification\" or \"segmentation\"\n",
    "#   DOMAIN (for classification): one of list(CONFIG.keys()) or \"all\"\n",
    "\n",
    "TASK = \"classification\"   # @param [\"classification\", \"segmentation\"]\n",
    "DOMAIN = \"all\"            # @param [\"all\", \"brain\", \"lungs\", \"skin\", \"breast\", \"bone\"]\n",
    "\n",
    "# Segmentation paths (used only if TASK == \"segmentation\")\n",
    "SEG_IMAGES = os.path.join(DATA_ROOT, \"brain_seg/train/images\")\n",
    "SEG_MASKS = os.path.join(DATA_ROOT, \"brain_seg/train/masks\")\n",
    "SEG_OUTPUT = \"trained/unet_brain_tumor.pth\"\n",
    "\n",
    "print(f\"Task: {TASK}\")\n",
    "if TASK == \"classification\":\n",
    "    print(f\"Domain: {DOMAIN}\")\n",
    "    if DOMAIN == \"all\":\n",
    "        for name, cfg in CONFIG.items():\n",
    "            train_model(name, cfg)\n",
    "    else:\n",
    "        if DOMAIN in CONFIG:\n",
    "            train_model(DOMAIN, CONFIG[DOMAIN])\n",
    "        else:\n",
    "            print(f\"[ERROR] Unknown domain: {DOMAIN}\")\n",
    "            print(f\"Available domains: {', '.join(CONFIG.keys())}\")\n",
    "else:\n",
    "    train_segmentation(\n",
    "        images_dir=SEG_IMAGES,\n",
    "        masks_dir=SEG_MASKS,\n",
    "        output_path=SEG_OUTPUT,\n",
    "        input_size=(256, 256),\n",
    "        epochs=20,\n",
    "        batch_size=4,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}